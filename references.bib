@article{pecora2015master,
  title={Master Stability Function for Globally Synchronized Systems},
  author={Pecora, Louis M and Carroll, Thomas L},
  journal={Encyclopedia of Computational Neuroscience},
  pages={1663--1672},
  year={2015},
  publisher={Springer}
}

@MISC {stack_question,
    TITLE = {Bifurcation analysis, limit cycle collapses on two symmetric fixed points},
    AUTHOR = {giangian (https://math.stackexchange.com/users/661101/giangian)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/3198238 (version: 2019-04-24)},
    EPRINT = {https://math.stackexchange.com/q/3198238},
    URL = {https://math.stackexchange.com/q/3198238}
}

@book{gantmacher2005matrix,
  title={Applications of the Theory of Matrices},
  author={Gantmacher, Feliks Ruvimovi{\v{c}} and Brenner, Joel Lee},
  year={2005},
  publisher={Courier Corporation}
}

@Inbook{Guckenheimer1983,
author="Guckenheimer, John
and Holmes, Philip",
title="Local Bifurcations",
bookTitle="Nonlinear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields",
year="1983",
publisher="Springer New York",
address="New York, NY",
pages="117--165",
abstract="In this chapter, we study the local bifurcations of vector fields and maps. As we have seen, systems of physical interest typically have parameters which appear in the defining systems of equations. As these parameters are varied, changes may occur in the qualitative structure of the solutions for certain parameter values. These changes are called bifurcations and the parameter values are called bifurcation values. To the extent possible, we develop in this chapter and Chapters 6 and 7, a systematic theory which describes and permits the analysis of the typical bifurcations one encounters. We pay careful attention to the examples introduced in Chapter 2 and use these to illustrate the theory that we present.",
isbn="978-1-4612-1140-2",
doi="10.1007/978-1-4612-1140-2_3",
url="https://doi.org/10.1007/978-1-4612-1140-2_3"
}


@article{hopfield1984continuous,
  title={Neurons with graded response have collective computational properties like those of two-state neurons},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={81},
  number={10},
  pages={3088--3092},
  year={1984},
  publisher={National Acad Sciences}
}

@book{marsland2011machine,
  title={Machine learning: an algorithmic perspective},
  author={Marsland, Stephen},
  year={2011},
  publisher={Chapman and Hall/CRC}
}



@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{angeli2004multigraph,
  title={Multi-stability in monotone input/output systems},
  author={Angeli, David and Sontag, Eduardo D},
  journal={Systems \& Control Letters},
  volume={51},
  number={3-4},
  pages={185--202},
  year={2004},
  publisher={Elsevier}
}

@book{StrogatzNonlinear,
  Author = {Steven H. Strogatz},
  Title = {Nonlinear Dynamics And Chaos: With Applications To Physics, Biology, Chemistry And Engineering (Studies in Nonlinearity)},
  Publisher = {CRC Press},
  Year = {1994},
  ISBN = {0201543443},
}

@book{smith2008monotone,
  title={Monotone dynamical systems: an introduction to the theory of competitive and cooperative systems},
  author={Smith, Hal L},
  number={41},
  year={2008},
  publisher={American Mathematical Soc.}
}

@article{angeli2003monotone,
  title={Monotone control systems},
  author={Angeli, David and Sontag, Eduardo D},
  journal={IEEE Transactions on automatic control},
  volume={48},
  number={10},
  pages={1684--1698},
  year={2003},
  publisher={IEEE}
}

@article{enciso2005monotone,
  title={Monotone systems under positive feedback: multistability and a reduction theorem},
  author={Enciso, German and Sontag, Eduardo D},
  journal={Systems \& control letters},
  volume={54},
  number={2},
  pages={159--168},
  year={2005},
  publisher={Elsevier}
}

@article{johansson2006clustering,
  title={Clustering of stored memories in an attractor network with local competition},
  author={Johansson, Christopher and Ekeberg, {\"O}rjan and Lansner, Anders},
  journal={International journal of neural systems},
  volume={16},
  number={06},
  pages={393--403},
  year={2006},
  publisher={World Scientific}
}

@article{lansner1989one,
  title={A one-layer feedback artificial neural network with a Bayesian learning rule},
  author={Lansner, Anders and Ekeberg, {\"O}rjan},
  journal={International journal of neural systems},
  volume={1},
  number={01},
  pages={77--87},
  year={1989},
  publisher={World Scientific}
}

@article{BCPNNRecurrentZero,
author = {Lansner, Anders and Ekeberg, Örjan},
title = {A ONE-LAYER FEEDBACK ARTIFICIAL NEURAL NETWORK WITH A BAYESIAN LEARNING RULE},
journal = {International Journal of Neural Systems},
volume = {01},
number = {01},
pages = {77-87},
year = {1989},
doi = {10.1142/S0129065789000499},

URL = { 
        https://doi.org/10.1142/S0129065789000499
    
},
eprint = { 
        https://doi.org/10.1142/S0129065789000499
    
}
,
    abstract = { A probabilistic artificial neural network is presented. It is of a one-layer, feedback-coupled type with graded units. The learning rule is derived from Bayes's rule. Learning is regarded as collecting statistics and recall as a statistical inference process. Units correspond to events and connections come out as compatibility coefficients in a logarithmic combination rule. The input to a unit via connections from other active units affects the a posteriori belief in the event in question. The new model is compared to an earlier binary model with respect to storage capacity, noise tolerance, etc. in a content addressable memory (CAM) task. The new model is a real time network and some results on the reaction time for associative recall are given. The scaling of learning and relaxation operations is considered together with issues related to representation of information in one-layer artificial neural networks. An extension with complex units is discussed. }
}




@ARTICLE{Grossbergstability, 
author={M. A. Cohen and S. Grossberg}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
title={Absolute stability of global pattern formation and parallel memory storage by competitive neural networks}, 
year={1983}, 
volume={SMC-13}, 
number={5}, 
pages={815-826}, 
keywords={neural nets;stability;stability;global pattern formation;parallel memory storage;competitive neural networks;symmetric interactions;global Lyapunov function;equilibrium set;Stability analysis;Pattern formation;Trajectory;Symmetric matrices;Equations;Neural networks;Mathematical model}, 
doi={10.1109/TSMC.1983.6313075}, 
ISSN={0018-9472}, 
month={Sep.},}

@article{sandberg2002bayesian,
  title={A Bayesian attractor network with incremental learning},
  author={Sandberg, Anders and Lansner, Anders and Petersson, Karl Magnus and Ekeberg.},
  journal={Network: Computation in neural systems},
  volume={13},
  number={2},
  pages={179--194},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{SANDBERG2000987,
title = "A palimpsest memory based on an incremental Bayesian learning rule",
journal = "Neurocomputing",
volume = "32-33",
pages = "987 - 994",
year = "2000",
issn = "0925-2312",
doi = "https://doi.org/10.1016/S0925-2312(00)00270-8",
url = "http://www.sciencedirect.com/science/article/pii/S0925231200002708",
author = "A. Sandberg and A. Lansner and K.M. Petersson and Ö. Ekeberg",
keywords = "Bayesian confidence propagation, Palimpsest memory",
abstract = "Capacity limited memory systems need to gradually forget old information in order to avoid catastrophic forgetting where all stored information is lost. This can be achieved by allowing new information to overwrite old, as in the so-called palimpsest memory. This paper describes a new such learning rule employed in an attractor neural network. The network does not exhibit catastrophic forgetting, has a capacity dependent on the learning time constant and exhibits recency effects in retrieval."
}

@book{sandberg2003bayesian,
  title={Bayesian attractor neural network models of memory},
  author={Sandberg, Anders}
}

@book{memorymolecules,
    abstract = {{Combining insights from both cognitive neuroscience and molecular biology, two
of the world's leading experts address memory from molecules and cells to
brain systems and cognition. What is memory and where in the brain is it
stored? How is memory storage accomplished? This book touches on these
questions and many more, showing how the recent convergence of psychology and
biology has resulted in an exciting new synthesis of knowledge about learning
and remembering. Memory: From Mind to Molecules is an ideal primer for courses
on learning and memory or for general readers who are interested in
discovering what is currently known about one of the basic aspects of human
existence.}},
    author = {Squire, Larry R. and Kandel, Eric R.},
    citeulike-article-id = {2967123},
    day = {01},
    edition = {2},
    howpublished = {Hardcover},
    isbn = {0981519415},
    keywords = {resources},
    month = jul,
    posted-at = {2009-05-04 18:41:47},
    priority = {2},
    publisher = {Roberts and Company Publishers},
    title = {{Memory: From Mind to Molecules}},
    year = {2008}
}


@article{sandberg2003working,
  title={A working memory model based on fast Hebbian learning},
  author={Sandberg, A and Tegn{\'e}r, Jesper and Lansner, Anders},
  journal={Network: Computation in Neural Systems},
  volume={14},
  number={4},
  pages={789--802},
  year={2003},
  publisher={Taylor \& Francis}
}

@InProceedings{MiniNonSPikingLasner,
author="Lansner, Anders
and Frans{\'e}n, Erik",
editor="Bower, James M.",
title="Improving the Realism of Attractor Models By using Cortical Columns as Functional Units",
booktitle="The Neurobiology of Computation",
year="1995",
publisher="Springer US",
address="Boston, MA",
pages="251--256",
abstract="Attractor network models of cortical associative memory functions [1, 2] have developed considerably over the past few years. Such models have been criticized on several points i.e. low storage capacity, slow convergence time and high firing rates, but computer simulations done by ourselves and others have refuted most of this [3, 4, 5]. A remaining drawback, however, of these models is their lack of a biologically reasonable connectivity in that they are usually based on a full and symmetric connectivity between neurons. The real cortical connectivity pattern is much more complex, extremely sparse (at least on the average) and it is also likely to be asymmetric in general. In the work presented here, the functional units are assumed to be cortical mini-columns instead of single cells as in our previous model [6]. Each mini-column corresponds to a single unit in an attractor artificial neural network (ANN). This makes the connectivity of the model look more like that of real cortex, since it becomes sparse, especially over long distances, and also asymmetric at the level of single cells. The aim of this work is to show that such a network can operate as an associative memory in much the same way as attractor ANN:s and previous models using realistic neurons and synapses with a simple connectivity [3, 4].",
isbn="978-1-4615-2235-5"
}

@article{lundqvist2018working,
  title={Working memory: delay activity, yes! persistent activity? Maybe not},
  author={Lundqvist, Mikael and Herman, Pawel and Miller, Earl K},
  journal={Journal of Neuroscience},
  volume={38},
  number={32},
  pages={7013--7019},
  year={2018},
  publisher={Soc Neuroscience}
}

@article{Shafi2007VariabilityIN,
  title={Variability in neuronal activity in primate cortex during working memory tasks.},
  author={Mouhshin Shafi and Yong-Di Zhou and Javier Quintana and Carmen Chow and Juan Fuster and Mark Bodner},
  journal={Neuroscience},
  year={2007},
  volume={146 3},
  pages={1082-108}
}

@article{LansnerFRC,
    author = {Lansner, Anders AND Marklund, Petter AND Sikström, Sverker AND Nilsson, Lars-Göran},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Reactivation in Working Memory: An Attractor Network Model of Free Recall},
    year = {2013},
    month = {08},
    volume = {8},
    url = {https://doi.org/10.1371/journal.pone.0073776},
    pages = {1-11},
    abstract = {The dynamic nature of human working memory, the general-purpose system for processing continuous input, while keeping no longer externally available information active in the background, is well captured in immediate free recall of supraspan word-lists. Free recall tasks produce several benchmark memory phenomena, like the U-shaped serial position curve, reflecting enhanced memory for early and late list items. To account for empirical data, including primacy and recency as well as contiguity effects, we propose here a neurobiologically based neural network model that unifies short- and long-term forms of memory and challenges both the standard view of working memory as persistent activity and dual-store accounts of free recall. Rapidly expressed and volatile synaptic plasticity, modulated intrinsic excitability, and spike-frequency adaptation are suggested as key cellular mechanisms underlying working memory encoding, reactivation and recall. Recent findings on the synaptic and molecular mechanisms behind early LTP and on spiking activity during delayed-match-to-sample tasks support this view.},
    number = {8},
    doi = {10.1371/journal.pone.0073776}
}

@article {Fuster652,
	author = {Fuster, Joaquin M. and Alexander, Garrett E.},
	title = {Neuron Activity Related to Short-Term Memory},
	volume = {173},
	number = {3997},
	pages = {652--654},
	year = {1971},
	doi = {10.1126/science.173.3997.652},
	publisher = {American Association for the Advancement of Science},
	abstract = {Nerve cells in the monkey{\textquoteright}s prefrontal cortex and nucleus medialis dorsalis of the thalamus show changes of firing frequency associated with the performance of a delayed response test. Most cells increase firing during the cue presentation period or at the beginning of the ensuing delay; spike discharge highler than that in intertrial periods is present in some cells throughout the delay. These changes are interpreted as suggestive evidence of a role of frontothalamic circuits in the attentive process involved in short-term memory},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/173/3997/652},
	eprint = {http://science.sciencemag.org/content/173/3997/652.full.pdf},
	journal = {Science}
}

@phdthesis{Fiebigphd2018,
  title={Active Memory Processing: on Multiple Time-scalesin Simulated Cortical Networkswith Hebbian Plasticity},
  author={Fiebig, Florian},
  year={2018},
  school={KTH Royal Institute of Technology}
}

@techreport{Frans13,
abstract = {Attractor network models of cortical associative memory functions have developed considerably over the past few years. Here we show that we can improve them further, in terms of correspondence with cortical connectivity, b y using multiple cells in lamina II/III of cortical columns connected by long-range bers as the functional unit in the network instead of just a single cell. The connectivity of the model then becomes more realistic, since the original dense and symmetric connectivity n o w m a y be sparse and strongly asymmetric at the cell-to-cell level. Our simulations show that this kind of network, with model neurons of the Hodgkin-Huxley type arranged in columns, can operate as an associative memory in much the same way as previous models having a simpler connectivity. Cell activities comply with experimental findings and reaction times are within biological and psychological limits. By introducing a scaling model we make it plausible that a network approaching experimentally reported neuron numbers and synaptic distributions also could work like the studied network.},
author = {Frans, Erik and Lansner, Anders},
file = {:home/gianlu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frans, Lansner - Unknown - A Model of Cortical Associative Memory Based on a Horizontal Network of Connected Columns.pdf:pdf},
title = {{A Model of Cortical Associative Memory Based on a Horizontal Network of Connected Columns}},
url = {https://www.nada.kth.se/utbildning/forsk.utb/avhandlingar/dokt/961018e.pdf}
}
